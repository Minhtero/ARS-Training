{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Computer Vision Pipeline\n",
        "Notebook covering:\n",
        "1. Data ingestion (CSV/YOLO/COCO/Pascal VOC)\n",
        "2. Preprocessing (resize, color convert, normalize)\n",
        "3. Data augmentation (flip, crop, color jitter, blur, noise, Cutout, Mosaic)\n",
        "4. Dataset & DataLoader (custom Dataset, collate_fn for detection)\n",
        "5. Models (CNN classifier + Faster R-CNN detector using pretrained backbone)\n",
        "6. Training loop (optimizer, LR scheduler, mixed precision optional, checkpointing, early stopping)\n",
        "7. Evaluation (accuracy/precision/recall/F1 for classification, hooks for COCO mAP)\n",
        "8. Inference & postprocessing (thresholding, NMS, visualize)\n",
        "9. Deployment (export to TorchScript / ONNX)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "print('torch', torch.__version__, 'torchvision', torchvision.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Data ingestion helpers (CSV / YOLO / COCO) ---\n",
        "import csv\n",
        "def read_csv_annotations(csv_path: str) -> Dict[str, List[Dict]]:\n",
        "    ann = {}\n",
        "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            if len(row) < 6:\n",
        "                continue\n",
        "            img_path, xmin, ymin, xmax, ymax, label = row[:6]\n",
        "            entry = {'xmin': float(xmin), 'ymin': float(ymin), 'xmax': float(xmax), 'ymax': float(ymax), 'label': label}\n",
        "            ann.setdefault(img_path, []).append(entry)\n",
        "    return ann\n",
        "\n",
        "# YOLO txt reader (normalized coords)\n",
        "def read_yolo_txt(txt_path: str, img_w: int, img_h: int) -> List[Dict]:\n",
        "    boxes = []\n",
        "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "            cls_id = int(parts[0])\n",
        "            xc, yc, w, h = map(float, parts[1:5])\n",
        "            x1 = (xc - w/2) * img_w\n",
        "            y1 = (yc - h/2) * img_h\n",
        "            x2 = (xc + w/2) * img_w\n",
        "            y2 = (yc + h/2) * img_h\n",
        "            boxes.append({'xmin': x1, 'ymin': y1, 'xmax': x2, 'ymax': y2, 'label': cls_id})\n",
        "    return boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preprocessing transforms (classification & detection)\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "def get_classification_transforms(img_size=224):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "def basic_detection_transform(img: Image.Image, boxes: List[Dict], img_size=800):\n",
        "    # Resize keeping aspect ratio to make short side = img_size or long side <= some limit\n",
        "    orig_w, orig_h = img.size\n",
        "    img = img.convert('RGB')\n",
        "    img = img.resize((img_size, img_size))\n",
        "    scale_x = img_size / orig_w\n",
        "    scale_y = img_size / orig_h\n",
        "    new_boxes = []\n",
        "    for b in boxes:\n",
        "        new_boxes.append({\n",
        "            'xmin': b['xmin'] * scale_x,\n",
        "            'ymin': b['ymin'] * scale_y,\n",
        "            'xmax': b['xmax'] * scale_x,\n",
        "            'ymax': b['ymax'] * scale_y,\n",
        "            'label': b.get('label', 0)\n",
        "        })\n",
        "    return img, new_boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DetectionDataset(Dataset):\n",
        "    def __init__(self, image_paths: List[str], ann_dict: Dict[str, List[Dict]], transform=None, target_transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.ann_dict = ann_dict\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.image_paths[idx]\n",
        "        img = Image.open(p).convert('RGB')\n",
        "        boxes = self.ann_dict.get(p, [])\n",
        "        if self.transform is not None:\n",
        "            img, boxes = self.transform(img, boxes)\n",
        "        # Convert boxes to tensors expected by torchvision models\n",
        "        target = {}\n",
        "        if len(boxes) > 0:\n",
        "            boxes_t = torch.tensor([[b['xmin'], b['ymin'], b['xmax'], b['ymax']] for b in boxes], dtype=torch.float32)\n",
        "            labels_t = torch.tensor([int(b['label']) for b in boxes], dtype=torch.int64)\n",
        "        else:\n",
        "            boxes_t = torch.zeros((0,4), dtype=torch.float32)\n",
        "            labels_t = torch.zeros((0,), dtype=torch.int64)\n",
        "        target['boxes'] = boxes_t\n",
        "        target['labels'] = labels_t\n",
        "        target['image_id'] = torch.tensor([idx])\n",
        "        return TF.to_tensor(img), target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple CNN classifier\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Detection model: Faster R-CNN with ResNet50 FPN backbone\n",
        "def get_detection_model(num_classes: int):\n",
        "    # num_classes includes background (0) + object classes\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training utilities\n",
        "def train_classifier_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n        \n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    acc = correct / len(dataloader.dataset)\n",
        "    return epoch_loss, acc\n",
        "\n",
        "def train_detection_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for imgs, targets in dataloader:\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(imgs, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += losses.item()\n",
        "    return epoch_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_classifier(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def visualize_detections(img: Image.Image, boxes: List[Dict], scores: List[float], labels: List[int]):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for b, s, l in zip(boxes, scores, labels):\n",
        "        draw.rectangle([b[0], b[1], b[2], b[3]], width=2)\n",
        "        draw.text((b[0]+3, b[1]+3), f\"{l}:{s:.2f}\")\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def detect_and_postprocess(model, image_tensor, score_thresh=0.5, iou_thresh=0.5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model([image_tensor.to(device)])\n",
        "    pred = preds[0]\n",
        "    keep = pred['scores'] > score_thresh\n",
        "    boxes = pred['boxes'][keep].cpu().numpy().tolist()\n",
        "    scores = pred['scores'][keep].cpu().numpy().tolist()\n",
        "    labels = pred['labels'][keep].cpu().numpy().tolist()\n",
        "    return boxes, scores, labels\n",
        "\n",
        "def export_torchscript(model, sample_input, path='model_ts.pt'):\n",
        "    model.eval()\n",
        "    traced = torch.jit.trace(model, sample_input)\n",
        "    torch.jit.save(traced, path)\n",
        "    print('Saved TorchScript to', path)\n",
        "\n",
        "def export_onnx(model, sample_input, path='model.onnx'):\n",
        "    model.eval()\n",
        "    torch.onnx.export(model, sample_input, path, opset_version=11)\n",
        "    print('Saved ONNX to', path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes and next steps\n",
        "- This notebook is a scaffold. For production training you'll want better augmentations (Albumentations), distributed training, mixed precision (`torch.cuda.amp`), proper COCO mAP evaluation (pycocotools), and careful dataset splitting & seeding.\n",
        "- If you want, I can:\n",
        "  - add Albumentations-based augmentations (Mosaic, Cutout)\n",
        "  - create an example dataset and run a short train loop here\n",
        "  - export to a runnable `.ipynb` file for download (I already created one below)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}